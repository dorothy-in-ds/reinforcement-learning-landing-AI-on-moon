{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import TensorDataset\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(1998)\n",
    "np.random.seed(1998)\n",
    "env.action_space.seed(1998)\n",
    "torch.manual_seed(1998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network initiated similar to 3/10/2021 OH\n",
    "#todo: try two layers\n",
    "class network(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        # hidden layer\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        #output layer\n",
    "        self.fc3 = nn.Linear(32,output_size)\n",
    "    def forward(self, x):\n",
    "        # Get intermediate outputs using hidden layer\n",
    "        #print('input to model', x)\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        #print('forward one layer', x)\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        #print('forward two layers', x)\n",
    "        # Get predictions using output layer\n",
    "        x = self.fc3(x)\n",
    "        #print('output layer', x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_experience(d,q_net,q_net_target,minibatch_size,gamma,lr):\n",
    "    #network settings\n",
    "    optimizer = torch.optim.SGD(q_net.parameters(), lr=lr)\n",
    "    minibatch = random.sample(d,minibatch_size) \n",
    "    next_states = np.array([x.s_next for x in minibatch])\n",
    "    rewards = np.array([x.reward for x in minibatch])\n",
    "    done = np.array([x.done for x in minibatch])\n",
    "    actions = np.array([x.action for x in minibatch])\n",
    "    states = np.array([x.s for x in minibatch])\n",
    "    #calculate y for minibatch transitions\n",
    "    next_states_values = q_net_target(torch.Tensor(next_states)).max(1).values.detach().numpy()\n",
    "#     print('rewards: ',rewards)\n",
    "#     print('done: ',done)\n",
    "#     print('next_states_values: ',next_states_values)\n",
    "    q_values_y = torch.Tensor(rewards + (gamma*(1-done)*next_states_values))\n",
    "    #get real q value\n",
    "    states = torch.Tensor(states)\n",
    "    states_values = q_net(states)\n",
    "#     print('states_values: ',states_values)\n",
    "    actions = torch.from_numpy(actions[:,None])\n",
    "#     print('actions: ',actions)\n",
    "    q_values = states_values.gather(1,actions).squeeze()\n",
    "#     print('q_values: ',q_values)\n",
    "#     print('q_values_y: ',q_values_y)\n",
    "    #calculate the loss\n",
    "    optimizer.zero_grad()\n",
    "    loss = F.mse_loss(q_values,q_values_y)\n",
    "    #perform gradient descent step on with respect to the network parameters (action-values in q)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return q_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_dqn(q_net,q_net_target,n_episodes = 1000,gamma = 0.98,epsilon = 0.7,lr = 0.001,converged = False,minibatch_size = 64):\n",
    "    #hyperparameters\n",
    "    #Initialize replay memory to capacity N\n",
    "    c_steps = 5 ### update target q net every c steps\n",
    "    transition = namedtuple('transition',['s', 'action', 'reward', 's_next', 'done'])\n",
    "    experiences = []\n",
    "    scores = []\n",
    "    n_steps = []\n",
    "    episode = 0\n",
    "    for _ in range(n_episodes):\n",
    "        episode += 1\n",
    "        if(episode%1000 == 0): print('episode: ', episode)\n",
    "        print(episode)\n",
    "        #initialize the first state and preprocess it \n",
    "        s = env.reset()\n",
    "        #print('state is: ',s)\n",
    "        preprocessed_s = torch.from_numpy(s).unsqueeze(0)\n",
    "        #print('preprocessed state is: ',preprocessed_s)\n",
    "        done = False\n",
    "        step = 0\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            step += 1\n",
    "            #epsilon greedy method to choose the action\n",
    "            if  np.random.random() < epsilon:\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                predictions = q_net(preprocessed_s).detach().numpy()\n",
    "                #print('state_action_values are: ',predictions)\n",
    "                a = np.argmax(predictions)\n",
    "            #print('the action chosen is:', a)\n",
    "            s_next,reward,done,info = env.step(a)\n",
    "            total_reward += reward\n",
    "            preprocessed_s_next = torch.from_numpy(s).unsqueeze(0)\n",
    "            #store transition in d. \n",
    "            #if false done due to max steps reached then change done to false\n",
    "            if done and step == env._max_episode_steps:\n",
    "                t = transition(s,a,reward,s_next,False)\n",
    "            else: \n",
    "                t = transition(s,a,reward,s_next,done)\n",
    "            experiences = experiences + [t]\n",
    "            #sample and learn a minibatch from experiences\n",
    "            if (len(experiences)>=minibatch_size):\n",
    "                q_net.load_state_dict(learn_experience(experiences,q_net,q_net_target,minibatch_size,gamma,lr).state_dict())\n",
    "            if step % c_steps == 0:\n",
    "                q_net_target.load_state_dict(q_net.state_dict())\n",
    "            s = s_next\n",
    "            preprocessed_s = preprocessed_s_next\n",
    "        scores = scores + [total_reward]\n",
    "        n_steps = n_steps +[step]\n",
    "        #if the past 100 scores have an average of 200 points then break\n",
    "#         if sum(scores[-100:])/100 >= 200: \n",
    "#             Converged = True\n",
    "#             break\n",
    "        #decay epsilon so that more exploration is allowed at the beginning and more exploitation is used later on when number pf episodes get big\n",
    "        epsilon *= 0.995\n",
    "        epsilon = max(epsilon, 0.05) \n",
    "    return scores, n_steps, episode, converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network settings\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#played the trained agent\n",
    "def play_dqn(q_net,n_episodes=500):\n",
    "    scores = []\n",
    "    n_steps = []\n",
    "    episode = 0\n",
    "    for _ in range(n_episodes):\n",
    "        episode += 1\n",
    "        print(episode)\n",
    "        #initialize the first state and preprocess it \n",
    "        s = env.reset()\n",
    "        #print('state is: ',s)\n",
    "        preprocessed_s = torch.from_numpy(s).unsqueeze(0)\n",
    "        #print('preprocessed state is: ',preprocessed_s)\n",
    "        done = False\n",
    "        step = 0\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            step += 1\n",
    "            #greedy method to choose the action\n",
    "            predictions = q_net(preprocessed_s).detach().numpy()\n",
    "            a = np.argmax(predictions)\n",
    "            #take the action\n",
    "            s_next,reward,done,info = env.step(a)\n",
    "            total_reward += reward\n",
    "            #update s \n",
    "            preprocessed_s_next = torch.from_numpy(s).unsqueeze(0)\n",
    "            s = s_next\n",
    "            preprocessed_s = preprocessed_s_next\n",
    "        scores = scores + [total_reward]\n",
    "        n_steps = n_steps +[step]\n",
    "        #if the past 100 scores have an average of 200 points then break\n",
    "    return scores, n_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop training as soon as achiving first 200 average score over 100 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#initialize Q and Q target, which are networks with 4 as the number of outputs and 8 as number of inputs\n",
    "q_net = network(input_size = 8, output_size = 4)\n",
    "q_net_target = network(input_size = 8, output_size = 4)\n",
    "q_net.to(device)\n",
    "q_net_target.to(device)\n",
    "scores, n_steps, episode, converged = train_dqn(n_episodes = 1000,epsilon = 1,q_net = q_net, q_net_target = q_net_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the scores\n",
    "plt.plot(np.arange(len(scores)),scores)\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('ith epsiode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "play_scores, n_steps = play_dqn(q_net = q_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the scores\n",
    "plt.plot(np.arange(len(play_scores)),play_scores)\n",
    "plt.ylabel('score of a trained agent')\n",
    "plt.xlabel('ith epsiode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(play_scores)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(q_net.state_dict(), 'immediate_stop_e-3_lr')\n",
    "# model = network(input_size = 8, output_size = 4)\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#initialize Q and Q target, which are networks with 4 as the number of outputs and 8 as number of inputs\n",
    "env = gym.make('LunarLander-v2')\n",
    "q_net_2 = network(input_size = 8, output_size = 4)\n",
    "q_net_target_2 = network(input_size = 8, output_size = 4)\n",
    "q_net_2.to(device)\n",
    "q_net_target_2.to(device)\n",
    "scores2, n_steps2, episode2, converged2 = train_dqn(n_episodes = 1000,epsilon = 1,q_net = q_net_2, q_net_target = q_net_target_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the scores\n",
    "#rolling_mean = numpy.std(rolling_window(np.array(scores2), 100), 1)\n",
    "plt.plot(np.arange(len(scores2)),scores2)\n",
    "#plt.plot(np.arange(len(scores2)), rolling_mean, color='red')\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('epsiode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_scores2, n_steps2 = play_dqn(q_net = q_net_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the scores\n",
    "plt.plot(np.arange(len(play_scores2)),play_scores2)\n",
    "plt.ylabel('score of a trained agent,2')\n",
    "plt.xlabel('ith epsiode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(play_scores2)/500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(q_net_2.state_dict(), '1000_episodes_e-3_lr')\n",
    "\n",
    "# model = network(input_size = 8, output_size = 4)\n",
    "# model.load_state_dict(torch.load('1000_episodes_e-3_lr'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grid search of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pool = [1e-1,1e-2,1e-3,1e-4,1e-5]\n",
    "batch_size_pool = [8,16,32,64,128]\n",
    "gamma_pool = [0.9,0.925,0.95,0.975,0.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method: train on the same number of episodes and \n",
    "column_names = ['learning_rate','batch_size','gamma','converged','scores']\n",
    "tuning = pd.DataFrame(columns = column_names)\n",
    "n_experiment = 0\n",
    "for learnr in lr_pool:\n",
    "    for batch_size in batch_size_pool:\n",
    "        for gamma in gamma_pool:\n",
    "            q_net = network(input_size = 8, output_size = 4)\n",
    "            q_net_target = network(input_size = 8, output_size = 4)\n",
    "            q_net.to(device)\n",
    "            q_net_target.to(device)\n",
    "            print('testing learning rate = ', learnr,'batch_size = ',batch_size, 'gamma=', gamma)\n",
    "            scores, n_steps, episode, converged = train_dqn(n_episodes = 2000,\n",
    "                                                 epsilon = 1,\n",
    "                                                 q_net = q_net, \n",
    "                                                 q_net_target = q_net_target,\n",
    "                                                 lr = learnr,\n",
    "                                                 gamma = gamma\n",
    "                                                )\n",
    "            tuning.loc[n_experiment] = [learnr,batch_size,gamma,converged,str(scores)]\n",
    "            print(tuning)\n",
    "            n_experiment += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A naive approach is to try a few different values and see which one gives you the best loss without sacrificing speed of training. We might start with a large value like 0.1, then try exponentially lower values: 0.01, 0.001, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the effect of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_effect = tuning.query('learning_rate == 0.001 and batch_size == 64')\n",
    "\n",
    "gamma0900 = np.array(eval(gamma_effect.iloc[0]['scores']))\n",
    "gamma0900_average = np.convolve(gamma0900, np.ones(100)/100, mode='valid')\n",
    "gamma0925 = np.array(eval(gamma_effect.iloc[1]['scores']))\n",
    "gamma0925_average = np.convolve(gamma0925, np.ones(100)/100, mode='valid')\n",
    "gamma095 = np.array(eval(gamma_effect.iloc[2]['scores']))\n",
    "gamma095_average = np.convolve(gamma095, np.ones(100)/100, mode='valid')\n",
    "gamma0975 = np.array(eval(gamma_effect.iloc[3]['scores']))\n",
    "gamma0975_average = np.convolve(gamma0975, np.ones(100)/100, mode='valid')\n",
    "gamma099 = np.array(eval(gamma_effect.iloc[4]['scores']))\n",
    "gamma099_average = np.convolve(gamma099, np.ones(100)/100, mode='valid')\n",
    "\n",
    "#plot the scores\n",
    "plt.plot(np.arange(100,1001),gamma0900_average, color ='blue', label = '0.900')\n",
    "plt.plot(np.arange(100,1001),gamma0925_average, color ='green', label = '0.925')\n",
    "plt.plot(np.arange(100,1001),gamma095_average, color ='red', label = '0.95')\n",
    "plt.plot(np.arange(100,1001),gamma0975_average, color ='purple', label = '0.975')\n",
    "plt.plot(np.arange(100,1001),gamma099_average, color ='skyblue', label = '0.99')\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('ith epsiode')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_effect = tuning.query('gamma == 0.99 and batch_size == 64')\n",
    "\n",
    "alpha1 = np.array(eval(alpha_effect.iloc[0]['scores'])[0:1000])\n",
    "alpha1_average = np.convolve(alpha1, np.ones(100)/100, mode='valid')\n",
    "alpha2 = np.array(eval(alpha_effect.iloc[1]['scores'])[0:1000])\n",
    "alpha2_average = np.convolve(alpha2, np.ones(100)/100, mode='valid')\n",
    "alpha3 = np.array(eval(alpha_effect.iloc[2]['scores'])[0:1000])\n",
    "alpha3_average = np.convolve(alpha3, np.ones(100)/100, mode='valid')\n",
    "alpha4 = np.array(eval(alpha_effect.iloc[3]['scores'])[0:1000])\n",
    "alpha4_average = np.convolve(alpha4, np.ones(100)/100, mode='valid')\n",
    "\n",
    "#plot the scores\n",
    "plt.plot(np.arange(100,1001),alpha1_average, color ='blue', label = '0.1')\n",
    "plt.plot(np.arange(100,1001),alpha2_average, color ='green', label = '0.01')\n",
    "plt.plot(np.arange(100,1001),alpha3_average, color ='red', label = '0.001')\n",
    "plt.plot(np.arange(100,1001),alpha4_average, color ='purple', label = '0.0001')\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('ith epsiode')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_batch_size = tuning.query('learning_rate == 0.001 and gamma == 0.99')\n",
    "\n",
    "batch8 = np.array(eval(tuning_batch_size.iloc[0]['scores']))\n",
    "batch8_average = np.convolve(batch8, np.ones(100)/100, mode='valid')\n",
    "batch16 = np.array(eval(tuning_batch_size.iloc[1]['scores']))\n",
    "batch16_average = np.convolve(batch16, np.ones(100)/100, mode='valid')\n",
    "batch32 = np.array(eval(tuning_batch_size.iloc[2]['scores']))\n",
    "batch32_average = np.convolve(batch32, np.ones(100)/100, mode='valid')\n",
    "batch64 = np.array(eval(tuning_batch_size.iloc[3]['scores']))\n",
    "batch64_average = np.convolve(batch64, np.ones(100)/100, mode='valid')\n",
    "batch128 = np.array(eval(tuning_batch_size.iloc[4]['scores']))\n",
    "batch128_average = np.convolve(batch128, np.ones(100)/100, mode='valid')\n",
    "\n",
    "#plot the scores\n",
    "plt.plot(np.arange(100,1001),batch8_average, color ='blue', label = '8')\n",
    "plt.plot(np.arange(100,1001),batch16_average, color ='green', label = '16')\n",
    "plt.plot(np.arange(100,1001),batch32_average, color ='red', label = '32')\n",
    "plt.plot(np.arange(100,1001),batch64_average, color ='purple', label = '64')\n",
    "plt.plot(np.arange(100,1001),batch128_average, color ='skyblue', label = '128')\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('ith epsiode')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
