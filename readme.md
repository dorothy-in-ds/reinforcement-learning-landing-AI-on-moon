# Using Deep Q Learning to Solve the Lunar Lander Problem
This project implemented the DQN algorithm from Mnih et. al's paper Human-level control through deep reinforcement learning. The DQN is used to train an agent in the lunar lander environment with a goal of achieving an average score of 200 over 100 episodes.  
## Running the experiments
### Step One
Before doing any experiment, run all the cells up to the "1 Stop training as soon as achiving first 200 average score over 100 episodes" markdown cell to activate the libraries, global variable, and function definitions. 
### To get the training and testing scores
Uncomment line 52 to line 54 in the function definition of "train_dqn", run all the cells below "1 Stop training as soon as achiving first 200 average score over 100 episodes" markdown cell and above "1.1 Play the trained agent". This will train the agent from scratch and stop training until the first occurance of an average score of 200 over 100 episodes. A plot will be outputed which records all the scores for every training episode. If you run the cells within the "1.1 Play the trained agent" markdown, this will let the abovementioned trained agent play the lunar lander environment and output the scores will all the testing episodes. Similarly, comment line 52 to line 54 in the function definition of "train_dqn", run all the cells below "2 Train 2000" markdown cell and above "2.1 Play the trained agent". This will train the agent from scratch for 1000 episodes. A plot will be outputed which records all the scores for every training episode. If you run the cells within the "2.1 Play the trained agent" markdown, this will let the abovementioned trained agent play the lunar lander environment and output the scores will all the testing episodes. 
### To get the visualizations for hyper parameter effects
Run all the cells within "3 gird search of hyperparameters" and "4 visualize the effect of hyperparameters". This will generate three plots detailing the effect of gamma, alpha, and mini batch size. 
